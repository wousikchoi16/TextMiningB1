{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9ca054",
   "metadata": {},
   "source": [
    "# Ch 16. 한글문서에 대한 BERT 활용\n",
    "\n",
    "\n",
    "## 16.1 다중 언어를 위한(multilingual) BERT 사전학습모형의 미세조정학습을 이용한 한글 문서의 분류\n",
    "\n",
    "\n",
    "(1) https://github.com/google-research/bert/blob/master/multilingual.md\n",
    "<br>(2) https://huggingface.co/transformers/pretrained_models.html\n",
    "<br>(3) https://github.com/SKTBrain/KoBERT\n",
    "<br>(4) https://github.com/Beomi/KcBERT\n",
    "<br>(5) https://github.com/Beomi/KcELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffaf66f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size: 8282\n",
      "#Validation set size: 2761\n",
      "#Test set size: 3682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "# rating이 6보다 작으면 0 즉 부정, 6 이상이면 긍정으로 라벨 생성\n",
    "y = [0 if rate < 6 else 1 for rate in df.rating]\n",
    "# 데이터셋을 학습, 검증, 평가의 세 데이터셋으로 분리\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df.review.tolist(), y, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=0)\n",
    "\n",
    "print('#Train set size:', len(X_train))\n",
    "print('#Validation set size:', len(X_val))\n",
    "print('#Test set size:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160ac054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5042229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a32c24d3f364c16896e81f7d687e6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450e1475ca154718bfdcd959ffbfb329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef2987341c24255a5614e69d7353b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안', '##녕', '##하', '##세', '##요', '.', '반', '##갑', '##습', '##니다', '.']\n",
      "{'input_ids': [101, 9521, 118741, 35506, 24982, 48549, 119, 9321, 118610, 119081, 48345, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n",
    "inputs = tokenizer(\"안녕하세요. 반갑습니다.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8780d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\wousi/.cache\\huggingface\\transformers\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at C:\\Users\\wousi/.cache\\huggingface\\transformers\\0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\Users\\wousi\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8282\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2072\n",
      "<ipython-input-2-c0002db1d308>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='251' max='2072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 251/2072 51:03 < 6:13:20, 0.08 it/s, Epoch 0.24/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification \n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 토큰화\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)\n",
    "\n",
    "# bert-base-multilingual-cased 사전학습모형으로부터 분류기 모형을 생성\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Trainer에서 사용할 하이퍼 파라미터 지정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 모형 예측이나 체크포인트 출력 폴더, 반드시 필요함\n",
    "    num_train_epochs=2,              # 학습 에포크 수\n",
    "    evaluation_strategy=\"steps\",      # epoch마다 검증 데이터셋에 대한 평가 지표를 출력\n",
    "    eval_steps = 500,                # \n",
    "    per_device_train_batch_size=8,   # 학습에 사용할 배치 사이즈\n",
    "    per_device_eval_batch_size=16,   # 평가에 사용할 배치 사이즈\n",
    "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    ")\n",
    "\n",
    "# Trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model=model,                     # 학습할 모형\n",
    "    args=training_args,              # 위에서 정의한 학습 매개변수\n",
    "    train_dataset=train_dataset,     # 훈련 데이터셋\n",
    "    eval_dataset=val_dataset,        # 검증 데이터셋\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 미세조정학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecaf4d91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to my_model\n",
      "Configuration saved in my_model\\config.json\n",
      "Model weights saved in my_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b83935",
   "metadata": {},
   "source": [
    "아래와 같이 미세조정학습을 마친 모형으로 평가 데이터셋에 대해 성능을 측정해본다. 80.1% 정도로 나쁘지 않은 성능을 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5512b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3682\n",
      "  Batch size = 16\n",
      "<ipython-input-2-c0002db1d308>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='231' max='231' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [231/231 34:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5428703427314758,\n",
       " 'eval_accuracy': 0.7645301466594242,\n",
       " 'eval_runtime': 2075.896,\n",
       " 'eval_samples_per_second': 1.774,\n",
       " 'eval_steps_per_second': 0.111,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b528db2",
   "metadata": {},
   "source": [
    "## 16.2 KoBERT 사전학습모형에 대한 파이토치 기반 미세조정학습\n",
    "\n",
    "https://github.com/SKTBrain/KoBERT\n",
    "\n",
    "\n",
    "https://github.com/SKTBrain/KoBERT/tree/master/kobert_hf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b4c0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to c:\\users\\wousi\\appdata\\local\\temp\\pip-req-build-oj5657fv\n",
      "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit e1f2f37055e7460d8427f6912579c0162cb69831\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.21.42-py3-none-any.whl (132 kB)\n",
      "Collecting gluonnlp>=0.6.0\n",
      "  Using cached gluonnlp-0.10.0.tar.gz (344 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting mxnet>=1.4.0\n",
      "  Using cached mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl (33.1 MB)\n",
      "Collecting onnxruntime==1.8.0\n",
      "  Using cached onnxruntime-1.8.0-cp38-cp38-win_amd64.whl (4.7 MB)\n",
      "Requirement already satisfied: sentencepiece>=0.1.6 in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from kobert==0.2.3) (0.1.96)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from kobert==0.2.3) (1.11.0)\n",
      "Requirement already satisfied: transformers>=4.8.1 in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from kobert==0.2.3) (4.18.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.20.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from onnxruntime==1.8.0->kobert==0.2.3) (2.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from onnxruntime==1.8.0->kobert==0.2.3) (3.19.1)\n",
      "Requirement already satisfied: cython in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (0.29.23)\n",
      "Requirement already satisfied: packaging in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (20.9)\n",
      "Collecting numpy>=1.16.6\n",
      "  Using cached numpy-1.16.6.zip (5.1 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting requests<2.19.0,>=2.18.4\n",
      "  Using cached requests-2.18.4-py2.py3-none-any.whl (88 kB)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Using cached graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from torch>=1.7.0->kobert==0.2.3) (3.7.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from transformers>=4.8.1->kobert==0.2.3) (2021.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from transformers>=4.8.1->kobert==0.2.3) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from transformers>=4.8.1->kobert==0.2.3) (0.0.49)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from transformers>=4.8.1->kobert==0.2.3) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from transformers>=4.8.1->kobert==0.2.3) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from transformers>=4.8.1->kobert==0.2.3) (4.62.3)\n",
      "Collecting transformers>=4.8.1\n",
      "  Using cached transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "  Using cached transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "  Using cached transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
      "  Using cached transformers-4.16.0-py3-none-any.whl (3.5 MB)\n",
      "  Using cached transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Collecting transformers>=4.8.1\n",
      "  Using cached transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
      "  Using cached transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
      "  Using cached transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.12.4-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.12.1-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.12.0-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "  Using cached transformers-4.11.2-py3-none-any.whl (2.9 MB)\n",
      "  Using cached transformers-4.11.1-py3-none-any.whl (2.9 MB)\n",
      "  Using cached transformers-4.11.0-py3-none-any.whl (2.9 MB)\n",
      "  Using cached transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in c:\\users\\wousi\\anaconda3\\lib\\site-packages (from transformers>=4.8.1->kobert==0.2.3) (0.5.1)\n",
      "  Using cached transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
      "  Using cached transformers-4.10.1-py3-none-any.whl (2.8 MB)\n",
      "  Using cached transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
      "  Using cached transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
      "Collecting huggingface-hub==0.0.12\n",
      "  Using cached huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Collecting transformers>=4.8.1\n",
      "  Using cached transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
      "  Using cached transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
      "  Using cached transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
      "  Using cached transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch>=1.7.0\n",
      "  Using cached torch-1.11.0-cp38-cp38-win_amd64.whl (158.0 MB)\n",
      "INFO: pip is looking at multiple versions of sentencepiece to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sentencepiece>=0.1.6\n",
      "  Using cached sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "INFO: pip is looking at multiple versions of mxnet to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mxnet>=1.4.0\n",
      "  Using cached mxnet-1.7.0.post1-py2.py3-none-win_amd64.whl (33.0 MB)\n",
      "  Using cached mxnet-1.6.0-py2.py3-none-win_amd64.whl (26.9 MB)\n",
      "  Using cached mxnet-1.5.0-py2.py3-none-win_amd64.whl (23.5 MB)\n",
      "  Using cached mxnet-1.4.1-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "  Using cached mxnet-1.4.0.post0-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "  Using cached mxnet-1.4.0-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "INFO: pip is looking at multiple versions of gluonnlp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gluonnlp>=0.6.0\n",
      "  Using cached gluonnlp-0.9.2.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of mxnet to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gluonnlp-0.9.1.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached gluonnlp-0.9.0.post0.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.3.tar.gz (236 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.2.tar.gz (237 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.1.tar.gz (236 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.0.tar.gz (235 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of sentencepiece to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of gluonnlp to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gluonnlp-0.7.1.tar.gz (233 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gluonnlp-0.7.0.tar.gz (233 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.6.0.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of onnxruntime to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of kobert to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    onnxruntime 1.8.0 depends on numpy>=1.16.6\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.4.0.post0 depends on numpy<1.15.0 and >=1.8.2\n",
      "    onnxruntime 1.8.0 depends on numpy>=1.16.6\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.4.0 depends on numpy<1.15.0 and >=1.8.2\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' 'C:\\Users\\wousi\\AppData\\Local\\Temp\\pip-req-build-oj5657fv'\n",
      "ERROR: Cannot install kobert because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece\n",
    "# !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
    "# !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
    "# !pip install ipywidgets  # for vscode\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195eaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n",
    "inputs = tokenizer(\"안녕하세요. 반갑습니다.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82152a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 토큰화\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# KoBERT 사전학습모형 로드\n",
    "bert_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "# BERT를 포함한 신경망 모형\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model, token_size, num_labels): \n",
    "        super(MyModel, self).__init__()\n",
    "        self.token_size = token_size\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_model = pretrained_model\n",
    "\n",
    "        # 분류기 정의\n",
    "        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # BERT 모형에 입력을 넣고 출력을 받음\n",
    "        outputs = self.pretrained_model(**inputs)\n",
    "        # BERT 출력에서 CLS 토큰에 해당하는 부분만 가져옴\n",
    "        bert_clf_token = outputs.last_hidden_state[:,0,:]\n",
    "        \n",
    "        return self.classifier(bert_clf_token)\n",
    "\n",
    "# token_size는 BERT 토큰과 동일, bert_model.config.hidden_size로 알 수 있음\n",
    "model = MyModel(bert_model, num_labels=2, token_size=bert_model.config.hidden_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94646422",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "from transformers import get_scheduler\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optim,\n",
    "    num_warmup_steps=200,\n",
    "    num_training_steps=total_training_steps\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee83e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# GPU 가속을 사용할 수 있으면 device를 cuda로 설정하고, 아니면 cpu로 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)  # 모형을 GPU로 복사\n",
    "model.train()     # 학습모드로 전환\n",
    "\n",
    "# 옵티마이저를 트랜스포머가 제공하는 AdamW로 설정\n",
    "optim = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01) # 가중치 감쇠 설정\n",
    "criterion = torch.nn.CrossEntropyLoss()    # 멀티클래스이므로 크로스 엔트로피를 손실함수로 사용\n",
    "\n",
    "num_epochs = 2      # 학습 epoch를 3회로 설정\n",
    "total_training_steps = num_epochs * len(train_loader)\n",
    "# 학습 스케줄러 설정\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optim,\n",
    "                                            num_training_steps=total_training_steps,\n",
    "                                            num_warmup_steps=200)\n",
    "\n",
    "start = time.time() # 시작시간 기록\n",
    "train_loss = 0\n",
    "eval_steps = 500\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #total_epoch_loss = 0  # epoch의 총 loss 초기화\n",
    "    for batch in train_loader:\n",
    "        model.train()     # 학습모드로 전환\n",
    "        optim.zero_grad()     # 그래디언트 초기화\n",
    "        # 배치에서 label을 제외한 입력만 추출하여  GPU로 복사\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'} \n",
    "        labels = batch['labels'].to(device) # 배치에서 라벨을 추출하여 GPU로 복사\n",
    "        outputs = model(inputs) # 모형으로 결과 예측\n",
    "        # 두 클래스에 대해 예측하고 각각 비교해야 하므로 labels에 대해 원핫인코딩을 적용한 후에 손실을 게산\n",
    "        loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # loss 계산\n",
    "\n",
    "        train_loss += loss\n",
    "        loss.backward() # 그래디언트 계산\n",
    "        optim.step()    # 가중치 업데이트\n",
    "        scheduler.step() # 스케줄러 업데이트\n",
    "        \n",
    "        step += 1\n",
    "        if step % eval_steps == 0: # eval_steps 마다 경과한 시간과 loss를 출력\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                model.eval()\n",
    "                for batch in val_loader:\n",
    "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # loss 계산\n",
    "                    val_loss += loss\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "            avg_train_loss = train_loss / eval_steps\n",
    "            elapsed = time.time() - start\n",
    "            print('Step %d, elapsed time: %.2f, train loss: %.4f, validation loss: %.4f' \n",
    "                  % (step, elapsed, avg_train_loss, avg_val_loss))\n",
    "            train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric= load_metric(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    with torch.no_grad(): # 학습할 필요가 없으므로 그래디언트 계산을 끔\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f60783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

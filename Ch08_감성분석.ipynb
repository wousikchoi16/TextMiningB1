{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. 감성 분석\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 감성 사전을 이용한 영화 리뷰 감성 분석\n",
    "\n",
    "### 8.2.1 NLTK 영화 리뷰 데이터 준비\n",
    "\n",
    "1: https://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\wousi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#review count: 2000\n",
      "#samples of file ids: ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
      "#categories of reviews: ['neg', 'pos']\n",
      "#num of \"neg\" reviews: 1000\n",
      "#num of \"pos\" reviews: 1000\n",
      "#id of the first review: neg/cv000_29416.txt\n",
      "#part of the first review: plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt\n",
      "#sentiment of the first review: ['neg']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "print('#review count:', len(movie_reviews.fileids())) #영화 리뷰 문서의 id를 반환\n",
    "print('#samples of file ids:', movie_reviews.fileids()[:10]) #id를 10개까지만 출력\n",
    "print('#categories of reviews:', movie_reviews.categories()) # label, 즉 긍정인지 부정인지에 대한 분류\n",
    "print('#num of \"neg\" reviews:', len(movie_reviews.fileids(categories='neg'))) #label이 부정인 문서들의 id를 반환\n",
    "print('#num of \"pos\" reviews:', len(movie_reviews.fileids(categories='pos'))) #label이 긍정인 문서들의 id를 반환\n",
    "\n",
    "fileid = movie_reviews.fileids()[0] #첫번째 문서의 id를 반환\n",
    "print('#id of the first review:', fileid)\n",
    "print('#part of the first review:', movie_reviews.raw(fileid)[:500]) #첫번째 문서의 내용을 500자까지만 출력\n",
    "print('#sentiment of the first review:', movie_reviews.categories(fileid)) #첫번째 문서의 감성\n",
    "\n",
    "fileids = movie_reviews.fileids() #movie review data에서 file id를 가져옴\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids] #file id를 이용해 raw text file을 가져옴\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[1]\n",
    "categories[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 TextBlob을 이용한 감성 분석\n",
    "\n",
    "1: https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://textblob.readthedocs.io/en/dev/quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob\n",
    "#### - 토큰화같은 전처리를 알아서 수행하기 때문에 텍스트를 인수로 바로 넘겨줌.\n",
    "#### - 토큰화, POS태깅, 명사추출, 문서분류(나이브베이즈, 결정트리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용법 \n",
    "#### 앞에 TextBlob.sentiment처럼 붙여줌. \n",
    "#### 1) sentiment : 긍정부정\n",
    "#### 2) words : 토큰화된 단어\n",
    "#### 3) sentence : 토큰화된 문장\n",
    "#### 4) tags : POS태깅의 결과\n",
    "#### 5) noun_phrases : 추출된 명사 리스트 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U textblob\n",
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn't snag this one correctly . \n",
      "they seem to have taken this pretty neat concept , but executed it terribly . \n",
      "so what are the problems with the movie ? \n",
      "well , its main problem is that it's simply too jumbled . \n",
      "it starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what's going on . \n",
      "there are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \n",
      "now i personally don't mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film's biggest problem . \n",
      "it's obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \n",
      "and do they make things entertaining , thrilling or even engaging , in the meantime ? \n",
      "not really . \n",
      "the sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn't the make the film all that more entertaining . \n",
      "i guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \n",
      "i mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \n",
      "okay , we get it . . . there \n",
      "are people chasing her and we don't know who they are . \n",
      "do we really need to see it over and over again ? \n",
      "how about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \n",
      "apparently , the studio took this film away from its director and chopped it up themselves , and it shows . \n",
      "there might've been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \n",
      "the actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \n",
      "but my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character's unraveling . \n",
      "overall , the film doesn't stick because it doesn't entertain , it's confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \n",
      "oh , and by the way , this is not a horror or teen slasher flick . . . it's \n",
      "just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \n",
      "it also wrapped production two years ago and has been sitting on the shelves ever since . \n",
      "whatever . . . skip \n",
      "it ! \n",
      "where's joblo coming from ? \n",
      "a nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \n",
      "\n",
      "Sentiment(polarity=0.06479782948532947, subjectivity=0.5188408350908352)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "result = TextBlob(reviews[0])\n",
    "print(result)\n",
    "print(result.sentiment) # polarity 양수면 긍정, 음수면 부정 subjectivity 의견이면 1, 사실이면 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_TextBlob(docs):\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        testimonial = TextBlob(doc)\n",
    "        if testimonial.sentiment.polarity > 0:\n",
    "            results.append('pos')\n",
    "        else:\n",
    "            results.append('neg')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories형식: 2000\n",
      "sentiment형식: 2000\n",
      "#TextBlob을 이용한 리뷰 감성분석의 정확도: 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"categories형식:\", len(categories)) # list형식(shape작동안함)\n",
    "print(\"sentiment형식:\", len(sentiment_TextBlob(reviews))) # list형식 \n",
    "# 정확도의 입력값은 list형식으로 입력\n",
    "print('#TextBlob을 이용한 리뷰 감성분석의 정확도:', accuracy_score(categories, sentiment_TextBlob(reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3 AFINN을 이용한 감성 분석\n",
    "\n",
    "https://github.com/fnielsen/afinn \n",
    "\n",
    "(1) http://corpustext.com/reference/sentiment_afinn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFINN\n",
    "#### - 별도의 전처리가 필요없고, 텍스트를 그대로 넣어줌. \n",
    "#### - score사용법이 TextBlob.sentiment.polarity랑 다르다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting afinn\n",
      "  Downloading afinn-0.1.tar.gz (52 kB)\n",
      "     ---------------------------------------- 52.6/52.6 KB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: afinn\n",
      "  Building wheel for afinn (setup.py): started\n",
      "  Building wheel for afinn (setup.py): finished with status 'done'\n",
      "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53447 sha256=2f9afd62d823d2297c71268a6c0e01cbf93a7eed0ae492bc8f3abccd1c165ca6\n",
      "  Stored in directory: c:\\users\\wousi\\appdata\\local\\pip\\cache\\wheels\\f6\\6f\\c3\\b305c5107a17618f2938a067d5ffcbb556909d82398762089e\n",
      "Successfully built afinn\n",
      "Installing collected packages: afinn\n",
      "Successfully installed afinn-0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Afinn을 이용한 리뷰 감성분석의 정확도: 0.664\n"
     ]
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "def sentiment_Afinn(docs):\n",
    "    afn = Afinn(emoticons=True)\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        if afn.score(doc) > 0:\n",
    "            results.append('pos')\n",
    "        else:\n",
    "            results.append('neg')\n",
    "    return results\n",
    "\n",
    "print('#Afinn을 이용한 리뷰 감성분석의 정확도:', accuracy_score(categories, sentiment_Afinn(reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.4 VADER를 이용한 감성 분석\n",
    "\n",
    "(1) https://github.com/cjhutto/vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\wousi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Vader을 이용한 리뷰 감성분석의 정확도: 0.635\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def sentiment_vader(docs):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        score = analyser.polarity_scores(doc)\n",
    "        if score['compound'] > 0:\n",
    "            results.append('pos')\n",
    "        else:\n",
    "            results.append('neg')\n",
    "\n",
    "    return results\n",
    "\n",
    "print('#Vader을 이용한 리뷰 감성분석의 정확도:', accuracy_score(categories, sentiment_vader(reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이외의 감성분석툴\n",
    "### - SentiWordNet\n",
    "### - LIWC(Linguistic Inquiry and Word Count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.5 한글 감성사전\n",
    "### - KoreanSentimentAnalyze 라이브러리 형태 존재 \n",
    "\n",
    "1: https://github.com/park1200656/KnuSentiLex   \n",
    "2: https://github.com/mrlee23/KoreanSentimentAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 학습을 통한 머신러닝 기반의 감성 분석\n",
    "\n",
    "### 8.3.1 NLTK 영화 리뷰에 대한 머신러닝 기반 감성 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count:  1600\n",
      "Test set count:  400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split #sklearn에서 제공하는 split 함수를 사용\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, categories, test_size=0.2, random_state=7)\n",
    "\n",
    "print('Train set count: ', len(X_train))\n",
    "print('Test set count: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set dimension: (1600, 36189)\n",
      "#Test set dimension: (400, 36189)\n",
      "#Train set score: 0.998\n",
      "#Test set score: 0.797\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB #sklearn이 제공하는 MultinomialNB 를 사용\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(X_train) \n",
    "\n",
    "X_train_tfidf = tfidf.transform(X_train) # train set을 변환\n",
    "print('#Train set dimension:', X_train_tfidf.shape) # 실제로 몇개의 특성이 사용되었는지 확인\n",
    "X_test_tfidf = tfidf.transform(X_test) # test set을 변환\n",
    "print('#Test set dimension:', X_test_tfidf.shape)\n",
    "\n",
    "NB_clf = MultinomialNB(alpha=0.01) # 분류기 선언\n",
    "NB_clf.fit(X_train_tfidf, y_train) #train set을 이용하여 분류기(classifier)를 학습\n",
    "print('#Train set score: {:.3f}'.format(NB_clf.score(X_train_tfidf, y_train))) #train set에 대한 예측정확도를 확인\n",
    "print('#Test set score: {:.3f}'.format(NB_clf.score(X_test_tfidf, y_test))) #test set에 대한 예측정확도를 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 다음 영화 리뷰에 대한 머신러닝 기반 감성 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재미있다</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "0                             돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29   \n",
       "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n",
       "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n",
       "3                                이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n",
       "4                                               재미있다      10  2018.10.20   \n",
       "\n",
       "    title  \n",
       "0  인피니티 워  \n",
       "1  인피니티 워  \n",
       "2  인피니티 워  \n",
       "3  인피니티 워  \n",
       "4  인피니티 워  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      646\n",
       "1     1178\n",
       "2      343\n",
       "3      379\n",
       "4      314\n",
       "5      686\n",
       "6      610\n",
       "7      892\n",
       "8     1287\n",
       "9     2057\n",
       "10    6333\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df.rating.value_counts().sort_index().plot(kind='bar')\n",
    "#df.rating.plot.hist(bins=10) #히스토그램을 그릴 수도 있다.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| | 긍정으로 예측한 리뷰(PP) | 부정으로 예측한 리뷰(PN) |\n",
    "|---|---|---|\n",
    "|실제 긍정인 리뷰(P) | True positive(TP) | False negative(FN) |\n",
    "|실제 부정인 리뷰(N) | False positive(FP) | True negative(TN) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size: 11043\n",
      "#Test set size: 3682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data and labels into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.review, df.rating, random_state=7)\n",
    "print('#Train set size:', len(X_train))\n",
    "print('#Test set size:', len(X_test))\n",
    "\n",
    "from konlpy.tag import Okt #konlpy에서 Twitter 형태소 분석기를 import\n",
    "#from konlpy.tag import Twitter #konlpy에서 Twitter 형태소 분석기를 import\n",
    "okt = Okt()\n",
    "\n",
    "def twit_tokenizer(text): #전체를 다 사용하는 대신, 명사, 동사, 형용사를 사용\n",
    "    target_tags = ['Noun', 'Verb', 'Adjective']\n",
    "    result = []\n",
    "    for word, tag in okt.pos(text, norm=True, stem=True):\n",
    "        if tag in target_tags:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, max_features=2000, min_df=5, max_df=0.5) #명사, 동사, 형용사를 이용하여 tfidf 생성\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11043x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 114096 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Regression Train set R2 score: 0.605\n",
      "#Regression Test set R2 score: 0.395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()  #객체를 생성\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "print('#Regression Train set R2 score: {:.3f}'.format(lr.score(X_train_tfidf, y_train)))\n",
    "print('#Regression Test set R2 score: {:.3f}'.format(lr.score(X_test_tfidf, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Accuracy for train set: 0.888\n",
      "#Precision for train set: 0.893\n",
      "#Recall for train set: 0.969\n",
      "#F1 for train set: 0.929\n",
      "#Accuracy for test set: 0.848\n",
      "#Precision for test set: 0.868\n",
      "#Recall for test set: 0.946\n",
      "#F1 for test set: 0.905\n"
     ]
    }
   ],
   "source": [
    "y_train_senti = (y_train > 5)\n",
    "y_test_senti = (y_test > 5)\n",
    "\n",
    "y_train_predict = (lr.predict(X_train_tfidf) > 5)\n",
    "y_test_predict = (lr.predict(X_test_tfidf) > 5)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('#Accuracy for train set: {:.3f}'.format(accuracy_score(y_train_senti, y_train_predict)))\n",
    "print('#Precision for train set: {:.3f}'.format(precision_score(y_train_senti, y_train_predict)))\n",
    "print('#Recall for train set: {:.3f}'.format(recall_score(y_train_senti, y_train_predict)))\n",
    "print('#F1 for train set: {:.3f}'.format(f1_score(y_train_senti, y_train_predict)))\n",
    "\n",
    "print('#Accuracy for test set: {:.3f}'.format(accuracy_score(y_test_senti, y_test_predict)))\n",
    "print('#Precision for test set: {:.3f}'.format(precision_score(y_test_senti, y_test_predict)))\n",
    "print('#Recall for test set: {:.3f}'.format(recall_score(y_test_senti, y_test_predict)))\n",
    "print('#F1 for test set: {:.3f}'.format(f1_score(y_test_senti, y_test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Accuracy for train set: 0.878\n",
      "#Precision for train set: 0.878\n",
      "#Recall for train set: 0.973\n",
      "#F1 for train set: 0.923\n",
      "#Accuracy for test set: 0.855\n",
      "#Precision for test set: 0.866\n",
      "#Recall for test set: 0.958\n",
      "#F1 for test set: 0.910\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression #sklearn이 제공하는 logistic regression을 사용\n",
    "\n",
    "#count vector에 대해 regression을 해서 NB와 비교\n",
    "LR_clf = LogisticRegression() #분류기 선언\n",
    "LR_clf.fit(X_train_tfidf, y_train_senti) # train data를 이용하여 분류기를 학습\n",
    "\n",
    "y_train_predict = LR_clf.predict(X_train_tfidf)\n",
    "y_test_predict = LR_clf.predict(X_test_tfidf)\n",
    "\n",
    "print('#Accuracy for train set: {:.3f}'.format(accuracy_score(y_train_senti, y_train_predict)))\n",
    "print('#Precision for train set: {:.3f}'.format(precision_score(y_train_senti, y_train_predict)))\n",
    "print('#Recall for train set: {:.3f}'.format(recall_score(y_train_senti, y_train_predict)))\n",
    "print('#F1 for train set: {:.3f}'.format(f1_score(y_train_senti, y_train_predict)))\n",
    "\n",
    "print('#Accuracy for test set: {:.3f}'.format(accuracy_score(y_test_senti, y_test_predict)))\n",
    "print('#Precision for test set: {:.3f}'.format(precision_score(y_test_senti, y_test_predict)))\n",
    "print('#Recall for test set: {:.3f}'.format(recall_score(y_test_senti, y_test_predict)))\n",
    "print('#F1 for test set: {:.3f}'.format(f1_score(y_test_senti, y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 참고문헌\n",
    "\n",
    "Finn Årup Nielsen A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings 93-98. 2011 May.\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
